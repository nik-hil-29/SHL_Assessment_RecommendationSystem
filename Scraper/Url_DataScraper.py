"""This file uses the the JSON files generated by the DataScraper.py 
and it iterates over each url and extract the description and assessment 
length for each assessment and recombine it."""

import json
import logging
import asyncio
import agentql
from playwright.async_api import async_playwright

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

# More comprehensive query to extract description and time from different possible locations
DETAIL_QUERY = """
{ 
  description
  assessment_time_duration
}
"""
import nest_asyncio

nest_asyncio.apply()

async def main():
    # Load the existing JSON data
    try:
        with open("data/shl_pre_packaged_data.json", "r") as f: ## change the path for individual test solution 
            data = json.load(f)
            log.info("Successfully loaded existing data file")
    except Exception as e:
        log.error(f"Could not load existing data file: {e}")
        return

    # Check if the loaded data is a list and iterate from data[0] to data[last_index]
    if isinstance(data, list):
        containers = data
    elif isinstance(data, dict):
        # If data is a dict, wrap it into a list for uniform processing.
        containers = [data]
    else:
        log.error("Data format is not supported")
        return

    log.info(f"Found {len(containers)} containers to process")
    # print(containers)
    async with async_playwright() as playwright:
        browser = await playwright.chromium.launch()
        context = await browser.new_context()
        
        container_index = 0
        for container in containers:
            container_index += 1
            # Get the list of items from the container.
            # If the key "Individual_Test_Solutions" exists, use it;
            # otherwise, assume the container itself is the list.
            if isinstance(container, dict) and "Pre_packaged_job_solutions" in container:
                items = container["Pre_packaged_job_solutions"]
                # print(items)
            elif isinstance(container, list):
                items = container
            else:
                log.warning(f"Container {container_index} does not contain individual test solutions")
                continue

            log.info(f"Processing container {container_index} with {len(items)} items")
            
            item_index = 0
            for item in items:
                # print(item)
                item_index += 1
                if "url" not in item:
                    log.warning(f"Container {container_index} - Item {item_index} missing URL")
                    continue
                    
                url = item["url"]
                name = item.get("name", f"Container {container_index} - Item {item_index}")
                # print(url)
                try:
                    log.info(f"Processing Container {container_index} - Item {item_index}: {name}")
                    
                    # Open a new page for each URL
                    page = await agentql.wrap_async(await context.new_page())
                    await page.goto(url)
                    print(url)
                    
                    # Extract the details using agentql
                    detail_data = await page.query_data(DETAIL_QUERY)
                    # print(detail_data)
                    log.info(f"Detail data keys: {detail_data.keys() if detail_data else 'None'}")
                    
                    # Add the details to the original item
                    item["description"] = detail_data.get("description")
                    item["assessment_time_duration"] = detail_data.get("assessment_time_duration")
                    
                    # If we didn't get specific fields but got page content, try extracting from it
                    if not item["description"] and "page_content" in detail_data:
                        content = detail_data["page_content"]
                        if isinstance(content, str) and "description" in content.lower():
                            parts = content.split("\n")
                            for j, part in enumerate(parts):
                                if "description" in part.lower() and j+1 < len(parts):
                                    item["description"] = parts[j+1]
                                    break
                    
                    # Close the page
                    await page.close()
                    
                    # Small delay to be polite to the server
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    log.error(f"Error processing {url}: {str(e)}")
                    item["description"] = None
                    item["assessment_time_duration"] = None

        # Save the enhanced data preserving the original structure
        with open("data_source/shl_enhanced_solutions_prepacksol.json", "w") as f:
            json.dump(data, f, indent=4)
        ##uncoment the below line for individual test solutions
        # with open("data_source/shl_enhanced_solution.json", "w") as f:
        #     json.dump(data, f, indent=4)
        log.info("Enhanced data saved to shl_enhanced_solutions_prepacksol.json")
        await browser.close()

# For Jupyter notebooks
async def run_scraper():
    await main()

# Run directly
if __name__ == "__main__":
    asyncio.run(main())
